
[model]
base_causal_model = gpt2
base_t5_model     = t5-small

[training]
batch_size = 8
gradient_accumulation_steps = 4
learning_rate = 5e-5
num_epochs = 3
max_seq_length = 256

[paths]
output_dir = ./outputs
lora_output_dir = ./outputs/lora_causal
t5_prefix_output_dir = ./outputs/t5_prefix

[hardware]
device = auto   ; auto / cuda / cpu
